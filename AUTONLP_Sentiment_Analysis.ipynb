{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4TZ0Qc7X6KX+YhqWpqS2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhuverma17/AUTOML/blob/main/AUTONLP_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Steps in an NLP Pipeline\n",
        "\n",
        "A typical **NLP pipeline** consists of several key steps that transform raw text data into structured output for analysis or model training. Below are the common steps involved:\n",
        "\n",
        "### 1. Text Collection and Data Preprocessing\n",
        "- **Data Collection**: Gathering raw text data from various sources like web scraping, APIs, databases, or uploaded datasets.\n",
        "- **Cleaning**: Removing unnecessary or irrelevant information (e.g., HTML tags, special characters, stop words).\n",
        "- **Lowercasing**: Converting all text to lowercase to avoid distinguishing between words based on case.\n",
        "- **Removing Noise**: Cleaning the data by removing numbers, punctuation, special characters, or irrelevant content.\n",
        "\n",
        "### 2. Tokenization\n",
        "- **Word Tokenization**: Splitting the text into individual words or tokens. For example, the sentence \"I love NLP\" would be tokenized into `[\"I\", \"love\", \"NLP\"]`.\n",
        "- **Sentence Tokenization**: Splitting the text into sentences for sentence-level tasks.\n",
        "- **Subword Tokenization**: Sometimes, especially with languages that have compound words (e.g., German), breaking the text into subwords may be useful.\n",
        "\n",
        "### 3. Stop Words Removal\n",
        "- Stop words are common words like \"and\", \"the\", \"is\" that do not contribute much meaning to the text analysis.\n",
        "- These words are usually removed unless they serve a specific purpose in the context.\n",
        "\n",
        "### 4. Text Normalization\n",
        "- **Stemming**: Reducing words to their root form (e.g., \"running\" -> \"run\").\n",
        "- **Lemmatization**: Converting words to their base or dictionary form (e.g., \"better\" -> \"good\"). Lemmatization is more sophisticated than stemming as it considers the context and part of speech.\n",
        "- **Spell Correction**: Correcting common spelling errors in the text.\n",
        "\n",
        "### 5. Part-of-Speech Tagging (POS Tagging)\n",
        "- Assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a sentence, which helps in understanding the grammatical structure and meaning.\n",
        "\n",
        "### 6. Named Entity Recognition (NER)\n",
        "- Identifying and classifying entities in the text (e.g., names of people, places, dates, organizations).\n",
        "- Example: In the sentence \"Apple is releasing a new product in New York on March 10th,\" \"Apple\" (organization), \"New York\" (location), and \"March 10th\" (date) are recognized.\n",
        "\n",
        "### 7. Vectorization/Feature Extraction\n",
        "- **Bag of Words (BoW)**: Representing text as a set of word counts or frequencies.\n",
        "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: A statistical measure to evaluate how important a word is to a document in a collection.\n",
        "- **Word Embeddings**: Converting words to dense vectors (e.g., Word2Vec, GloVe, FastText), capturing semantic meaning and relationships between words.\n",
        "- **Transformers-based Embeddings**: Using models like BERT, GPT, or T5 to get contextualized word embeddings.\n",
        "\n",
        "### 8. Text Representation (Optional)\n",
        "- Depending on the task, you may also want to represent the entire document or sentence using embeddings like Doc2Vec, sentence embeddings, or transformer-based models like BERT for more advanced NLP tasks.\n",
        "\n",
        "### 9. Modeling\n",
        "- **Supervised Learning**: Training a model for tasks like classification, regression, or sequence labeling (e.g., sentiment analysis, text classification, named entity recognition).\n",
        "- **Unsupervised Learning**: Techniques like clustering (e.g., K-means) or topic modeling (e.g., Latent Dirichlet Allocation - LDA).\n",
        "- **Deep Learning**: Using advanced architectures like LSTM, GRU, BERT, or GPT for more complex tasks.\n",
        "\n",
        "### 10. Evaluation\n",
        "- Evaluating model performance using metrics like accuracy, precision, recall, F1-score for classification tasks, or BLEU score for text generation tasks.\n",
        "- **Cross-validation** may be used to assess the model's robustness and generalization.\n",
        "\n",
        "### 11. Post-Processing\n",
        "- **Text Generation or Transformation**: Generating text, summarization, translation, or other transformations based on the model's output.\n",
        "- **Filtering/Thresholding**: For tasks like sentiment analysis, setting a threshold to classify text as positive or negative.\n",
        "\n",
        "### 12. Deployment and Monitoring\n",
        "- Once the model is trained and evaluated, it is deployed into a production environment.\n",
        "- **Real-time inference**: Text data is passed through the model for prediction.\n",
        "- **Monitoring**: Tracking the model's performance over time to ensure it continues to work effectively, including retraining with new data as necessary.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The NLP pipeline typically starts with **data collection and preprocessing**, followed by **tokenization**, **stop words removal**, and **text normalization**. Then, it proceeds to **feature extraction** and **modeling**, followed by **evaluation**. Finally, it may include **post-processing** and **deployment** for real-world applications."
      ],
      "metadata": {
        "id": "3QHLyBumskA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS AUTOMML USES TYPICAL MACHINE LEARNING FRAMEWORK NOT DEEP LEARNING"
      ],
      "metadata": {
        "id": "mg28RLVBs8ne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gk1mxBOg-Ct"
      },
      "outputs": [],
      "source": [
        "!pip install autoviml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from autoviml import Auto_NLP\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KIcNg60yhJ5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets, info = tfds.load('imdb_reviews', with_info=True,batch_size=-1)"
      ],
      "metadata": {
        "id": "_FLS9df9tEQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datasets to DataFrames\n",
        "train_df = pd.DataFrame(datasets['train'])\n",
        "test_df = pd.DataFrame(datasets['test'])\n",
        "train_df['text'] = train_df['text'].astype(str)\n",
        "test_df['text'] = test_df['text'].astype(str)\n",
        "\n",
        "# Map the label: 1 = Positive, 0 = Negative\n",
        "train_df['label'] = train_df['label'].map({1: 'Positive', 0: 'Negative'})\n",
        "test_df['label'] = test_df['label'].map({1: 'Positive', 0: 'Negative'})\n",
        "\n",
        "# Let's inspect the first few rows of the training set\n",
        "print(train_df.head())\n",
        "\n",
        "# Step 2: Split train_df into training and validation sets (for K-Fold Cross Validation)\n",
        "input_columns = ['text']\n",
        "target_column = 'label'\n",
        "\n",
        "# Step 3: Set up K-Fold Cross Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "fold = 1\n",
        "validation_results = []  # To store validation results for each fold\n",
        "\n",
        "# Loop through each fold\n",
        "for train_index, val_index in kf.split(train_df):\n",
        "    print(f\"Training fold {fold}...\")\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    train_fold = train_df.iloc[train_index]\n",
        "    val_fold = train_df.iloc[val_index]\n",
        "\n",
        "    # Set up Auto_NLP pipeline for this fold\n",
        "    auto_nlp = Auto_NLP(nlp_column='text',  # Text column\n",
        "                        train=train_fold,   # Training dataset\n",
        "                        test=val_fold,      # Validation dataset\n",
        "                        target=target_column,verbose=2)  # Target column\n",
        "\n",
        "    # Train the model on the current fold's training data\n",
        "    model = auto_nlp.fit()\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    val_preds = model.predict(val_fold)\n",
        "\n",
        "    # Calculate accuracy for this fold\n",
        "    accuracy = accuracy_score(val_fold[target_column], val_preds)\n",
        "    validation_results.append(accuracy)\n",
        "\n",
        "    print(f\"Validation Accuracy for fold {fold}: {accuracy}\")\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "# After K-Fold Cross-Validation, evaluate the best model on the test set\n",
        "test_preds = best_model.predict(test_df)"
      ],
      "metadata": {
        "id": "9jPa_7lJtVR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Calculate confusion matrix on the test set\n",
        "conf_matrix = confusion_matrix(test_df[target_column], test_preds)\n",
        "\n",
        "# Step 5: Display confusion matrix\n",
        "print(\"Confusion Matrix on the Test Set:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Plot confusion matrix\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Optionally, calculate other performance metrics (precision, recall, F1-score)\n",
        "print(classification_report(test_df[target_column], test_preds))"
      ],
      "metadata": {
        "id": "5Y8akHHVt1Ib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}